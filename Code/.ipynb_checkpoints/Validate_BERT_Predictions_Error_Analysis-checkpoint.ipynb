{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The following functions, although used for generating train / test datasets, are left in here so the test files can be generated if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the files for parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to get tokens & their attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    offset = 0\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for token in tokens:\n",
    "        offset = text.find(token, offset)\n",
    "        start, end = offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "        start_indices.append(start)\n",
    "        end_indices.append(end)\n",
    "    return tokens, start_indices, end_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to generate IO Coding (Model2)\n",
    "\n",
    "IO coding is a technique for extracting entities, where in the input sentence is tokenized and analyzed for occurences of words that belong to an entity of interest.  It follows the following scheme:\n",
    "\n",
    "* I - marks beginning/inside of the entity\n",
    "* O - marks that the token is NOT part of any entity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_IO_Coding(file_path, tag, attribute):\n",
    "    \"\"\"\n",
    "    Function defined to take in a file for processing, tag and attributes to identify\n",
    "    within the file (xml).  The tags are first read into objects, capturing the label,\n",
    "    start and end values.  These are references in the 'text' object, where based on\n",
    "    which the annotators did identify the tag in context. \n",
    "    \n",
    "    This information is used to parse through the text (as tokens) and perform the BIO\n",
    "    coding based on the start position matches.\n",
    "    \n",
    "    Input: \n",
    "    file_path: path of the file to be read in for processing\n",
    "    tag: tag, as identified in the annotation.  Ex: DIABETES, HYPERTENSION etc.\n",
    "    attribute: specific attribute within the tag, from which to extract the value from\n",
    "    \n",
    "    Returns: \n",
    "    list of tokens, list of labels (IO coding)\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = ET.ElementTree(file=file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    text = root.find('TEXT').text.lower()\n",
    "    \n",
    "    tokens, start, end = spans(text)\n",
    "    labels_list = []\n",
    "    label_start = []\n",
    "    label_end = []\n",
    "    filename = []\n",
    "\n",
    "\n",
    "    for item in root.find(\"TAGS\"):\n",
    "        if item.tag  == tag:\n",
    "            label = (item.tag + \".\" + item.attrib[attribute]).lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            label = \"\"\n",
    "\n",
    "        for sub_item in item.findall(item.tag):\n",
    "            if 'start' in sub_item.attrib.keys():\n",
    "                labels_list.append(label)\n",
    "                label_start.append(int(sub_item.attrib['start']))\n",
    "                label_end.append(int(sub_item.attrib['end']))\n",
    "\n",
    "    bio_labels = []\n",
    "\n",
    "    count = 0\n",
    "    while len(start) > count:\n",
    "        if start[count] in label_start:\n",
    "            label_start_index = label_start.index(start[count])\n",
    "            end_index = label_end[label_start_index]\n",
    "            word_label = labels_list[label_start_index]\n",
    "            phrase = text[start[count]:end_index]\n",
    "            phrase_tokens = nltk.word_tokenize(phrase)\n",
    "            # update the tag to 'I-' so that this generates IO-Coding\n",
    "            next_tag=\"I-\"\n",
    "            for word in phrase_tokens:\n",
    "                if (len(word_label) > 0):\n",
    "                    new_label = next_tag + word_label\n",
    "                    bio_labels.append(new_label)\n",
    "                    next_tag=\"I-\"\n",
    "                else:\n",
    "                    bio_labels.append(\"O\")\n",
    "                count += 1\n",
    "        else:\n",
    "            bio_labels.append(\"O\")\n",
    "            count += 1 \n",
    "\n",
    "    # build this list to hold name of the file the token belongs to\n",
    "    # this is for the purpose of evaluation of the model from test results\n",
    "    for i in range(0, len(tokens)):\n",
    "        filename.append(file_path)\n",
    "        \n",
    "    return filename, tokens, bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIOCoding_data(tag, attrib, filenames):\n",
    "\n",
    "    \"\"\"\n",
    "    All files in the list (which holds the list of files in the directory) are parsed through\n",
    "    and the Generate_BIO_Coding function is called by passing individual files within the folder.\n",
    "    \n",
    "    The tags and attributes are passed on to the function as parameters.\n",
    "    \n",
    "    Input: \n",
    "    filenames: names of the file to be read in for processing in a list object\n",
    "    tag: tag, as identified in the annotation.  Ex: DIABETES, HYPERTENSION etc. (string)\n",
    "    attribute: specific attribute within the tag, from which to extract the value from (string)\n",
    "    \n",
    "    Returns: \n",
    "    list of tokens, list of labels (BIO coding, done across all files in the path for the tag/attribute)\n",
    "    \"\"\"\n",
    "    \n",
    "    test_tokens = []\n",
    "    test_labels = []\n",
    "    test_filenames = []\n",
    "\n",
    "    for file in filenames:\n",
    "        #print(\"processing file ... \", file)\n",
    "        filename, tokens, labels = Generate_IO_Coding(file_path=file, tag=tag, attribute=attrib)\n",
    "\n",
    "        test_tokens.extend(tokens)\n",
    "        test_labels.extend(labels)\n",
    "        test_filenames.extend(filename)  # this is for the purpose of validating test results\n",
    "        \n",
    "\n",
    "    return test_filenames, test_tokens, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form Test Data\n",
    "\n",
    "In the script in cell below, we have chosen the test xml files that start with '11'.  This is just to do a sample prediction against the model built using BERT as classifier.\n",
    "\n",
    "The condition should be removed to generate / test the complete set across all XML files in the 'testing' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to the appropriate folder on your local drive\n",
    "wd = os.path.dirname(os.path.abspath('__file__'))\n",
    "datafolder = [\"\\\\Dataset\\\\testing-RiskFactors-Complete\\\\\"]\n",
    "#print (datafolder)\n",
    "\n",
    "testfilenames = []\n",
    "\n",
    "for folder in datafolder:\n",
    "    for file in os.listdir(str(wd)+folder):\n",
    "        filename = os.fsdecode(os.fsencode((str(wd)+folder+file))) \n",
    "        if filename.endswith( ('.xml') ): # select xml files\n",
    "            #print(filename)\n",
    "            testfilenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Do not use as gold dataset does not have start and end attributes in tags \n",
    "    and hence cannot be parsed using the above function\n",
    "'''\n",
    "\n",
    "# set to the appropriate folder on your local drive\n",
    "wd = os.path.dirname(os.path.abspath('__file__'))\n",
    "datafolder = [\"\\\\Dataset\\\\testing-RiskFactors-Gold\\\\\"]\n",
    "#print (datafolder)\n",
    "\n",
    "goldfilenames = []\n",
    "\n",
    "for folder in datafolder:\n",
    "    for file in os.listdir(str(wd)+folder):\n",
    "        filename = os.fsdecode(os.fsencode((str(wd)+folder+file))) \n",
    "        if filename.endswith( ('.xml') ): # select xml files\n",
    "            #print(filename)\n",
    "            goldfilenames.append(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Test Tokens for 'Hypertension' (to test bert output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for model #5\n",
    "tag = 'HYPERTENSION'\n",
    "attribute = 'indicator'\n",
    "\n",
    "test_hypertension_indicator_filenames, test_hypertension_indicator_tokens, test_hypertension_indicator_labels = getIOCoding_data(tag, attribute, testfilenames)\n",
    "gold_hypertension_indicator_filenames, gold_hypertension_indicator_tokens, gold_hypertension_indicator_labels = getIOCoding_data(tag, attribute, goldfilenames)\n",
    "#hypertension_indicator_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture data into dataframe to work with it\n",
    "test_df_hypertension = pd.DataFrame({'filename': test_hypertension_indicator_filenames, 'test_token': test_hypertension_indicator_tokens, 'test_label': test_hypertension_indicator_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O                         380516\n",
       "I-hypertension.high_bp       733\n",
       "I-hypertension.mention       696\n",
       "Name: test_label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual counts of labels in test set for hypertension mention and high_bp\n",
    "test_df_hypertension['test_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture gold data into dataframe to work with it\n",
    "gold_df_hypertension = pd.DataFrame({'filename': gold_hypertension_indicator_filenames, 'gold_token': gold_hypertension_indicator_tokens, 'gold_label': gold_hypertension_indicator_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    381945\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual counts of labels in gold set for hypertension mention and high_bp\n",
    "gold_df_hypertension['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** All the labels in the gold set are getting the value 'O' because there are not \"start\" and \"end\" positions specified in the tags in the gold dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts of Labels from BERT Classifier\n",
    "\n",
    "Value counts of labels from BERT classifier (manually obtained from test_results file which holds probabilities for each class) is obtained as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(fullpath):\n",
    "    return fullpath.replace(\"C:\\\\Users\\\\sudha\\\\Documents\\\\W266-NLP\\\\Final-Project-W266\\\\Code\\\\Dataset\\\\testing-RiskFactors-Complete\\\\\", \"\")\n",
    "    #return fullpath.replace(\"C:\\\\Users\\\\Kalyan\\\\Documents\\\\Anu\\\\W266 - NLP\\\\Final Project\\\\lheart-disease-risk-prediction\\\\Code\\\\Dataset\\\\testing-RiskFactors-Gold\\\\\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_bert_results(tag, attribute, testfilenames):\n",
    "#def compare_with_bert_results(tag, attribute, testfilenames, goldfilenames):\n",
    "    # get data for model #5\n",
    "\n",
    "    \n",
    "    test_filenames, test_tokens, test_labels = getIOCoding_data(tag, attribute, testfilenames)\n",
    "    \n",
    "    # capture data into dataframe to work with it\n",
    "    test_df = pd.DataFrame({'filename': test_filenames, 'test_token': test_tokens, 'test_label': test_labels})\n",
    "\n",
    "    print(\"\\nThe value counts by labels: \", test_df['test_label'].value_counts(), \"\\n\\n\")\n",
    "    \n",
    "    #number of labels for the tag / attribute\n",
    "    num_labels = (test_df['test_label'].unique()).shape[0]\n",
    "    \n",
    "    tdf = test_df\n",
    "    tdf['file'] = tdf['filename'].apply(get_filename)\n",
    "    tdf.drop('filename', 1, inplace=True)\n",
    "    #print(tdf.head(10))\n",
    "\n",
    "    '''\n",
    "    # get gold data to validate\n",
    "    gold_filenames, gold_tokens, gold_labels = getIOCoding_data(tag, attribute, goldfilenames)\n",
    "    \n",
    "    # capture data into dataframe to work with it\n",
    "    gold_df = pd.DataFrame({'filename': gold_filenames, 'gold_token': gold_tokens, 'gold_label': gold_labels})\n",
    "\n",
    "    print(\"\\nThe value counts by labels: \", gold_df['gold_label'].value_counts(), \"\\n\\n\")\n",
    "    \n",
    "    #number of labels for the tag / attribute\n",
    "    num_labels = (gold_df['gold_label'].unique()).shape[0]\n",
    "    \n",
    "    gdf = gold_df\n",
    "    gdf['file'] = gdf['filename'].apply(get_filename)\n",
    "    gdf.drop('filename', 1, inplace=True)\n",
    "    #print(gdf.head(10))\n",
    "    '''\n",
    "\n",
    "    file_path=\"bert_output_results/\"+tag.lower()+\"/test_results.tsv\"\n",
    "    bert_results = pd.read_csv(file_path, sep='\\t',header=None)\n",
    "\n",
    "    class_list=[]\n",
    "    for i in range(0,num_labels):\n",
    "        class_list.append(\"Class-\"+str(i))\n",
    "        \n",
    "    # set the column names for the dataframe that holds BERT output\n",
    "    bert_results.columns=class_list\n",
    "    bert_results['classLabel'] = bert_results.idxmax(axis=1)\n",
    "    #print(\"BERT Results: \\n\", bert_results.head(10))\n",
    "    print(\"\\nBERT Results value counts by labels: \", bert_results['classLabel'].value_counts(), \"\\n\\n\")\n",
    "    \n",
    "    test_combined = pd.concat([tdf, bert_results['classLabel']], axis=1)\n",
    "    #test_combined = pd.concat([tdf, gdf, bert_results['classLabel']], axis=1)\n",
    "    \n",
    "    return test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The value counts by labels:  O                         380516\n",
      "I-hypertension.high_bp       733\n",
      "I-hypertension.mention       696\n",
      "Name: test_label, dtype: int64 \n",
      "\n",
      "\n",
      "\n",
      "BERT Results value counts by labels:  Class-2    380916\n",
      "Class-1       785\n",
      "Class-0       252\n",
      "Name: classLabel, dtype: int64 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get results for HYPERTENSION - Indicator\n",
    "tag = 'HYPERTENSION'\n",
    "attribute = 'indicator'\n",
    "\n",
    "df_HT_results = compare_with_bert_results(tag, attribute, testfilenames)\n",
    "\n",
    "#df_combined = compare_with_bert_results(tag, attribute, testfilenames, goldfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_label</th>\n",
       "      <th>test_token</th>\n",
       "      <th>file</th>\n",
       "      <th>classLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>record</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O</td>\n",
       "      <td>date</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O</td>\n",
       "      <td>:</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O</td>\n",
       "      <td>2069-04-07</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O</td>\n",
       "      <td>mr.</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O</td>\n",
       "      <td>villegas</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>O</td>\n",
       "      <td>is</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>seen</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O</td>\n",
       "      <td>today</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "      <td>C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...</td>\n",
       "      <td>Class-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_label  test_token                                               file  \\\n",
       "0          O      record  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "1          O        date  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "2          O           :  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "3          O  2069-04-07  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "4          O         mr.  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "5          O    villegas  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "6          O          is  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "7          O        seen  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "8          O       today  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "9          O           .  C:\\Users\\Kalyan\\Documents\\Anu\\W266 - NLP\\Final...   \n",
       "\n",
       "  classLabel  \n",
       "0    Class-2  \n",
       "1    Class-2  \n",
       "2    Class-2  \n",
       "3    Class-2  \n",
       "4    Class-2  \n",
       "5    Class-2  \n",
       "6    Class-2  \n",
       "7    Class-2  \n",
       "8    Class-2  \n",
       "9    Class-2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_HT_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_label', 'test_token', 'file', 'classLabel']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_HT_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test_Label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2441\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test_Label'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-e653d2f331ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_HT_results\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mdf_HT_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_Label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'I-hypertension.high_bp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1962\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1964\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1966\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1970\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1645\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3589\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2442\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2443\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2446\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test_Label'"
     ]
    }
   ],
   "source": [
    "df_HT_results[ df_HT_results['test_Label'] == 'I-hypertension.high_bp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_label    785\n",
       "test_token    785\n",
       "file          785\n",
       "classLabel    785\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_HT_results[ df_HT_results['classLabel'] == 'Class-1'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Mappings for Hypertension  are as follows:\n",
    "\n",
    "Class1 --> I-hypertension.high_bp\n",
    "\n",
    "Class2 --> I-hypertension.mention\n",
    "\n",
    "Class3 --> O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HT_set_labels(classlabel):\n",
    "    if (classlabel=='Class-0'):\n",
    "        return 'I-hypertension.mention'\n",
    "    elif (classlabel=='Class-1'):\n",
    "        return 'I-hypertension.high_bp'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HT_results['PredictedLabel'] = df_HT_results['classLabel'].apply(HT_set_labels)\n",
    "#df_HT_results.drop('classLabel', 1, inplace=True)\n",
    "\n",
    "df_HT_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DI_set_labels(classlabel):\n",
    "    if (classlabel=='Class0'):\n",
    "        return 'I-diabetes.a1c'\n",
    "    elif (classlabel=='Class1'):\n",
    "        return 'I-diabetes.glucose'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_DI_results['PredictedLabel'] = bert_DI_results['classLabel'].apply(DI_set_labels)\n",
    "bert_DI_results.drop('classLabel', 1, inplace=True)\n",
    "\n",
    "\n",
    "bert_DI_results.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run counts for Diabetes in TEST Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for model #5\n",
    "tag = 'DIABETES'\n",
    "attribute = 'indicator'\n",
    "\n",
    "test_diabetes_indicator_filenames, test_diabetes_indicator_tokens, test_diabetes_indicator_labels = getIOCoding_data(tag, attribute, testfilenames)\n",
    "gold_diabetes_indicator_filenames, gold_diabetes_indicator_tokens, gold_diabetes_indicator_labels = getIOCoding_data(tag, attribute, goldfilenames)\n",
    "#diabetes_indicator_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture data into dataframe to work with it\n",
    "test_df_diabetes = pd.DataFrame({'filename': test_diabetes_indicator_filenames, 'test_token': test_diabetes_indicator_tokens, 'test_label': test_diabetes_indicator_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual counts of labels in test set for diabetes mention, glucose and a1c\n",
    "test_df_diabetes['test_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture data into dataframe to work with it\n",
    "gold_df_diabetes = pd.DataFrame({'filename': gold_diabetes_indicator_filenames, 'gold_token': gold_diabetes_indicator_tokens, 'gold_label': gold_diabetes_indicator_labels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual counts of labels in test set for diabetes mention, glucose and a1c\n",
    "gold_df_diabetes['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results for DIABETES - Indicator\n",
    "tag = 'CAD'\n",
    "attribute = 'indicator'\n",
    "\n",
    "df_combined = compare_with_bert_results(tag, attribute, testfilenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[df_combined['classLabel'] != 'Class-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[1110:1125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running manual count checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_diabetes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LABELS for TOKENS in TEST Dataset against BERT Outputs\n",
    "\n",
    "BERT Classifier has returned results for the tokens passed in 'test.tsv' file.  The returned values are probabilities, that need to be converted into equivalent class labels based on majority class.  Then, the class label should be compared against the actual label from the code above to extract the IO-Coding from the xml files.  This is a brute-force approach or a manual way of verifying the validity of the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in results from BERT Predicitons to the above dataset\n",
    "The above dataset is derived from IO-Coding applied as done on the training set. This is what should be based on the annotation process. Nowe, we have to read in the predictions from bert, which is a set of class probabilities across all 3 classes and we have to merget that with the above dataset for comparison and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the test results captured for BERT Hypertension model and specify columns as the actual file has no header\n",
    "bert_DI_results = pd.read_csv(\"bert_output_results/diabetes/test_results.tsv\", sep='\\t',header=None)\n",
    "bert_DI_results.columns=[\"Class0\", \"Class1\", \"Class2\", \"Class3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_DI_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_DI_results['classLabel'] = bert_DI_results.idxmax(axis=1)\n",
    "\n",
    "bert_DI_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_DI_results['classLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DI_set_labels(classlabel):\n",
    "    if (classlabel=='Class0'):\n",
    "        return 'I-diabetes.a1c'\n",
    "    elif (classlabel=='Class1'):\n",
    "        return 'I-diabetes.glucose'\n",
    "    elif (classlabel=='Class2'):\n",
    "        return 'I-diabetes.mention'\n",
    "    else:\n",
    "        return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_DI_results['PredictedLabel'] = bert_DI_results['classLabel'].apply(DI_set_labels)\n",
    "bert_DI_results.drop('classLabel', 1, inplace=True)\n",
    "\n",
    "\n",
    "bert_DI_results.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the counts by label\n",
    "bert_DI_results['PredictedLabel'].value_counts()\n",
    "\n",
    "test_DI_combined = pd.concat([tdf, bert_DI_results['PredictedLabel']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[test_DI_combined['test_label']!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[test_DI_combined['PredictedLabel']!='O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[2220:2230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[376900:376910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[3080:3090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DI_combined[2460:2470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking I-hypertension.mention labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[75:85]\n",
    "# bert output predicted I-hypertension.mention for token in position 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[710:715]\n",
    "# bert output predicted I-hypertension.mention for token in position 712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[1990:2000]\n",
    "# bert output predicted I-hypertension.mention for token in position 1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[3080:3090]\n",
    "# bert output predicted I-hypertension.mention for token in position 3083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[6720:6730]\n",
    "# bert output predicted I-hypertension.mention for token in position 6726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking I-hypertension.high_bp labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[445:455]\n",
    "# bert output predicted I-hypertension.high_bp for token in position 451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[790:800]\n",
    "# bert output predicted I-hypertension.high_bp for token in position 794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[2360:2370]\n",
    "# bert output predicted I-hypertension.high_bp for token in position 2366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[12570:12580]\n",
    "# bert output predicted I-hypertension.high_bp for token in position 12574 and 12576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[18405:18418]\n",
    "# bert output predicted I-hypertension.high_bp for token in position 18415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Labeling by XML File\n",
    "\n",
    "Using the dataset captured above, extract information on the count of I-hypertension.mention and I-hypertension.high_bp tags in each of the files passed in test dataset and map them to the corresponding TAG and INDICATOR values.  This gives a high level counts of the tags identified in each of the files included in the test data.  This will be useful for error analysis and should be a base point for constructing the tags and start/end points if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.groupby(['file', 'test_label']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts by labels for hypertension mention and high_bp across all test files\n",
    "tdf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv('test_results_withfilenames.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in results from BERT Predicitons to the above dataset\n",
    "\n",
    "The above dataset is derived from IO-Coding applied as done on the training set. This is what should be based on the annotation process. Nowe, we have to read in the predictions from bert, which is a set of class probabilities across all 3 classes and we have to merget that with the above dataset for comparison and error analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the test results captured for BERT Hypertension model and specify columns as the actual file has no header\n",
    "bert_hypertension_results = pd.read_csv(\"bert_output_results/hypertension/bert_run1_test_results.tsv\", sep='\\t',header=None)\n",
    "bert_hypertension_results.columns=[\"Class1\", \"Class2\", \"Class3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hypertension_results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Mappings\n",
    "\n",
    "Class labels correspond as follows:\n",
    "\n",
    "* Class1 --> I-hypertension.high_bp\n",
    "* Class2 --> I-hypertension.mention\n",
    "* Class3 --> O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest = np.array(bert_hypertension_results)\n",
    "ntest.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  bert_hypertension_results['classLabel'] = bert_hypertension_results.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hypertension_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_labels(classlabel):\n",
    "    if (classlabel=='Class1'):\n",
    "        return 'I-hypertension.high_bp'\n",
    "    elif (classlabel=='Class2'):\n",
    "        return 'I-hypertension.mention'\n",
    "    else:\n",
    "        return 'O'\n",
    "\n",
    "bert_hypertension_results['PredictedLabel'] = bert_hypertension_results['classLabel'].apply(set_labels)\n",
    "bert_hypertension_results.drop('classLabel', 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hypertension_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating the counts by label\n",
    "bert_hypertension_results['PredictedLabel'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypertension_combined = pd.concat([tdf, bert_hypertension_results['PredictedLabel']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypertension_combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing (spot-checking) where model predicted labels 1 & 2\n",
    "test_hypertension_combined[70:85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypertension_combined[485:495]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypertension_combined[450:460]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hypertension_combined[790:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the predictions compared against actual test labels\n",
    "\n",
    "As seen above, BERT predictions seem very accurate and it seems to predict only after it has seen the complete context.  Also, punctuation marks are not labeled as one of the relevant classes, although a human annotator has done based on the instructions provided as part of the annotation process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
