{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8205963428680683788\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10193742398\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12125900850465017671\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the files for parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-05.xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set to the appropriate folder on your local drive\n",
    "codefolder = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "datafolders = ['\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\','\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\']\n",
    "filenames = []\n",
    "\n",
    "for folder in datafolders:\n",
    "    for file in os.listdir(str(codefolder)+folder):\n",
    "        filename=os.fsdecode(os.fsencode((str(codefolder)+folder+file)))\n",
    "        if filename.endswith(('.xml')):\n",
    "            filenames.append(filename)\n",
    "\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a 80/20 split\n",
    "split_index = int(len(filenames)*0.8)\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)\n",
    "\n",
    "train_filenames = filenames[:split_index]\n",
    "dev_filenames = filenames[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\103-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\109-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\155-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\251-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\107-02.xml']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\278-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\362-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\177-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\189-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\181-03.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\111-01.xml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafolder = '\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\'\n",
    "test_filenames = []\n",
    "\n",
    "for file in os.listdir(str(codefolder)+datafolder):\n",
    "    filename=os.fsdecode(os.fsencode((str(codefolder)+datafolder+file)))\n",
    "    if filename.endswith(('.xml')):\n",
    "        test_filenames.append(filename)\n",
    "\n",
    "test_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to get tokens & their attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    offset = 0\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for token in tokens:\n",
    "        offset = text.find(token, offset)\n",
    "        start, end = offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "        start_indices.append(start)\n",
    "        end_indices.append(end)\n",
    "    return tokens, start_indices, end_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to generate IO Coding - Many Models\n",
    "IO coding is a technique for extracting entities, where in the input sentence is tokenized and analyzed for occurences of words that belong to an entity of interest.  It follows the following scheme:\n",
    "\n",
    "* I - marks beginning/inside of the entity\n",
    "* O - marks that the token is NOT part of any entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_IO_Coding(file_path, tag, attribute):\n",
    "    \"\"\"\n",
    "    Function defined to take in a file for processing, tag and attributes to identify\n",
    "    within the file (xml).  The tags are first read into objects, capturing the label,\n",
    "    start and end values.  These are references in the 'text' object, where based on\n",
    "    which the annotators did identify the tag in context. \n",
    "    \n",
    "    This information is used to parse through the text (as tokens) and perform the BIO\n",
    "    coding based on the start position matches.\n",
    "    \n",
    "    Input: \n",
    "    file_path: path of the file to be read in for processing\n",
    "    tag: tag, as identified in the annotation.  Ex: DIABETES, HYPERTENSION etc.\n",
    "    attribute: specific attribute within the tag, from which to extract the value from\n",
    "    \n",
    "    Returns: \n",
    "    list of tokens, list of labels (IO coding)\n",
    "    \"\"\"\n",
    "    \n",
    "    tree = ET.ElementTree(file=file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    text = root.find('TEXT').text.lower()\n",
    "    \n",
    "    tokens, start, end = spans(text)\n",
    "    labels_list = []\n",
    "    label_start = []\n",
    "    label_end = []\n",
    "    filename = []\n",
    "\n",
    "\n",
    "    for item in root.find(\"TAGS\"):\n",
    "        if item.tag  == tag:\n",
    "            label = (item.tag + \".\" + item.attrib[attribute]).lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            label = \"\"\n",
    "\n",
    "        for sub_item in item.findall(item.tag):\n",
    "            if 'start' in sub_item.attrib.keys():\n",
    "                labels_list.append(label)\n",
    "                label_start.append(int(sub_item.attrib['start']))\n",
    "                label_end.append(int(sub_item.attrib['end']))\n",
    "\n",
    "        bio_labels = []\n",
    "\n",
    "        count = 0\n",
    "\n",
    "    while len(start) > count:\n",
    "        if start[count] in label_start:\n",
    "            label_start_index = label_start.index(start[count])\n",
    "            end_index = label_end[label_start_index]\n",
    "            word_label = labels_list[label_start_index]\n",
    "            phrase = text[start[count]:end_index]\n",
    "            phrase_tokens = nltk.word_tokenize(phrase)\n",
    "            # update the tag to 'I-' so that this generates IO-Coding\n",
    "            next_tag=\"I-\"\n",
    "            for word in phrase_tokens:\n",
    "                if (len(word_label) > 0):\n",
    "                    new_label = next_tag + word_label\n",
    "                    bio_labels.append(new_label)\n",
    "                    next_tag=\"I-\"\n",
    "                else:\n",
    "                    bio_labels.append(\"O\")\n",
    "                count += 1\n",
    "        else:\n",
    "            bio_labels.append(\"O\")\n",
    "            count += 1 \n",
    "\n",
    "    # build this list to hold name of the file the token belongs to\n",
    "    # this is for the purpose of evaluation of the model from test results\n",
    "    for i in range(0, len(tokens)):\n",
    "        filename.append(file_path)\n",
    "        \n",
    "    return filename, tokens, bio_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIOCoding_data(tag, attrib, filenames):\n",
    "\n",
    "    \"\"\"\n",
    "    All files in the list (which holds the list of files in the directory) are parsed through\n",
    "    and the Generate_BIO_Coding function is called by passing individual files within the folder.\n",
    "    \n",
    "    The tags and attributes are passed on to the function as parameters.\n",
    "    \n",
    "    Input: \n",
    "    filenames: names of the file to be read in for processing in a list object\n",
    "    tag: tag, as identified in the annotation.  Ex: DIABETES, HYPERTENSION etc. (string)\n",
    "    attribute: specific attribute within the tag, from which to extract the value from (string)\n",
    "    \n",
    "    Returns: \n",
    "    list of tokens, list of labels (BIO coding, done across all files in the path for the tag/attribute)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "\n",
    "    for file in filenames:\n",
    "        #print(\"processing file ... \", file)\n",
    "        filename, tokens, bio_labels = Generate_IO_Coding(file_path=file, tag=tag, attribute=attrib)\n",
    "\n",
    "        all_tokens.extend(tokens)\n",
    "        all_labels.extend(bio_labels)\n",
    "        all_filenames.extend(filename)  # this is for the purpose of validating test results\n",
    "        #print(\"finished processing file \", file, \"; and token length is \", len(all_tokens), \"; and label length: \", len(all_labels))\n",
    "        \n",
    "    return all_filenames, all_tokens, all_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Models\n",
    "\n",
    "The goal is to generate the tokens and the corresponding labels specific to the \"specializer\" model of interest.  The `TAG` - `attribute` pairs for the various specializer models are listed below:\n",
    "\n",
    "* **Model 1:** `DIABETES` - `indicator`\n",
    "* **Model 2:** `DIABETES` - `time`\n",
    "* **Model 3:** `CAD` - `indicator`\n",
    "* **Model 4:** `CAD` - `time`\n",
    "* **Model 5:** `HYPERTENSION` - `indicator`\n",
    "* **Model 6:** `HYPERTENSION` - `time`\n",
    "* **Model 7:** `HYPERLIPIDEMIA` - `indicator`\n",
    "* **Model 8:** `HYPERLIPIDEMIA` - `time`\n",
    "* **Model 9:** `SMOKER` - `status`\n",
    "* **Model 10:** `OBESE` - `indicator`\n",
    "* **Model 11:** `OBESE` - `time`\n",
    "* **Model 12:** `FAMILY_HIST` - `indicator`\n",
    "* **Model 13:** `MEDICATION` - `type1`\n",
    "* **Model 14:** `MEDICATION` - `time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(tag, attribute, filenames, input_type=\"train\"):\n",
    "    files, tokens, labels = getIOCoding_data(tag, attribute, filenames)\n",
    "    df = pd.DataFrame({'filename': files, 'token': tokens, 'label': labels})\n",
    "    \n",
    "    if (input_type == \"train\") or (input_type == \"dev\"):\n",
    "        le = LabelEncoder()\n",
    "        # Train - CoLA Format\n",
    "        df_bert = pd.DataFrame({'user_id':df.index, \n",
    "                                'label':le.fit_transform(df['label']), \n",
    "                                'alpha':['a']*df.shape[0], \n",
    "                                'text':df['token'].replace(r'\\n',' ', regex=True)})\n",
    "    \n",
    "    elif input_type == \"test\":\n",
    "        # Test -  CoLA Format\n",
    "        df_bert = pd.DataFrame({'id':df.index, \n",
    "                                'sentence':df['token'].replace(r'\\n', ' ', regex=True)})\n",
    "    \n",
    "    path = os.path.join(os.path.dirname(os.path.abspath('__file__')), tag + \"_\" + attribute)\n",
    "    if os.path.exists(path) == False:\n",
    "        os.makedirs(path)\n",
    "    df_bert.to_csv(os.path.join(path, input_type+\".tsv\"), sep='\\t', index=False, header=False)\n",
    "    return df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [['DIABETES', 'indicator'], ['DIABETES', 'time'], \n",
    "          ['CAD', 'indicator'], ['CAD', 'time'], \n",
    "          ['HYPERTENSION', 'indicator'], ['HYPERTENSION', 'time'],\n",
    "         ['HYPERLIPIDEMIA', 'indicator'], ['HYPERLIPIDEMIA', 'time'],\n",
    "         ['SMOKER', 'status'],\n",
    "         ['OBESE', 'indicator'], ['OBESE', 'time'],\n",
    "         ['FAMILY_HIST', 'indicator'],\n",
    "         ['MEDICATION', 'type1'], ['MEDICATION', 'time']]\n",
    "\n",
    "input_types = [['train', train_filenames], ['dev', dev_filenames], ['test', test_filenames]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    for input_type in input_types:\n",
    "        preprocess_input(model[0], model[1], input_type[1], input_type=input_type[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
