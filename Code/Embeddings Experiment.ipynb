{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, FlairEmbeddings, BertEmbeddings, CharacterEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import xml.etree.cElementTree as ET\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk.data\n",
    "from nltk import sent_tokenize\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7301"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce GTX TITAN X'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flair Text Classification\n",
    "\n",
    "## Data Format\n",
    "```\n",
    "__label__<label_1> <text>\n",
    "__label__<label_1> __label__<label_2> <text>\n",
    "```\n",
    "\n",
    "The text classification data format is based on the `FastText format`, in which each line in the file represents a texxt document. A document can have one or multiple labels that are defined at the beginning of the line starting with the prefix `__label__`. \n",
    "\n",
    "To create a `TaggedCorpus` for a text classification task, you need to have three files (train, dev, and test) in the above format located in one folder. This data folder structure could, for example, look like this:\n",
    "\n",
    "```\n",
    "/resources/tasks/project/train.txt\n",
    "/resources/tasks/project/dev.txt\n",
    "/resources/tasks/imdb/test.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get files for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-05.xml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codefolder = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "datafolders = ['\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\','\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\']\n",
    "filenames = []\n",
    "\n",
    "for folder in datafolders:\n",
    "    for file in os.listdir(str(codefolder)+folder):\n",
    "        filename=os.fsdecode(os.fsencode((str(codefolder)+folder+file)))\n",
    "        if filename.endswith(('.xml')):\n",
    "            filenames.append(filename)\n",
    "            \n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a 90/10 split\n",
    "split_index = int(len(filenames)*0.9)\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)\n",
    "\n",
    "train_filenames = filenames[:split_index]\n",
    "dev_filenames = filenames[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = '\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\'\n",
    "test_filenames = []\n",
    "\n",
    "for file in os.listdir(str(codefolder)+datafolder):\n",
    "    filename=os.fsdecode(os.fsencode((str(codefolder)+datafolder+file)))\n",
    "    if filename.endswith(('.xml')):\n",
    "        test_filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse sentences & labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(file):\n",
    "\n",
    "    tree = ET.ElementTree(file=file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    text = root.find('TEXT').text\n",
    "    sentences = [sent.split('\\n') for sent in sent_tokenize(text) if sent!='\\n']\n",
    "    all_sentences = []\n",
    "\n",
    "    for item in sentences:\n",
    "        for sub_item in item:\n",
    "            if sub_item.replace(' ','') != '':\n",
    "                all_sentences.append(sub_item)    \n",
    "    print(\"all_sentences:\",all_sentences[:5])\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file):\n",
    "    \n",
    "    # get all sentences in the file\n",
    "    tree = ET.ElementTree(file=file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    text = root.find('TEXT').text\n",
    "    sentences = [sent.split('\\n') for sent in sent_tokenize(text) if sent!='\\n']\n",
    "    all_sentences = []\n",
    "\n",
    "    for item in sentences:\n",
    "        for sub_item in item:\n",
    "            if sub_item.replace(' ','') != '':\n",
    "                all_sentences.append(sub_item)    \n",
    "                \n",
    "    #all_sent = get_sentences(file)\n",
    "    sent_label = {}\n",
    "\n",
    "    sub_tags = []\n",
    "    for item in root.find(\"TAGS\"):\n",
    "        if item.tag == 'PHI':\n",
    "            pass\n",
    "        elif item.tag == 'SMOKER':\n",
    "            label = (item.tag + \".\" + item.attrib['status']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'FAMILY_HIST':\n",
    "            label = (item.tag + \".\" + item.attrib['indicator']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'MEDICATION':\n",
    "            label = (item.tag + \".\" + item.attrib['type1'] + \".\" + item.attrib['type2'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            label = (item.tag + \".\" + item.attrib['indicator'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "        for sub_item in item.findall(item.tag):\n",
    "            #print(\"sub_item:\", sub_item.tag)\n",
    "            if ('text' in sub_item.attrib.keys()):\n",
    "                #print(sub_item.attrib['text'])\n",
    "                sub_tags.append((sub_item.attrib['text'], label))\n",
    "\n",
    "\n",
    "    count=0\n",
    "    for sent in all_sentences:\n",
    "        label='Other'\n",
    "        for tag in set(sub_tags):                                                       \n",
    "            if tag[0] in sent:\n",
    "                label = tag[1]\n",
    "                count += 1\n",
    "\n",
    "        sent_label[sent] = label\n",
    "        \n",
    "    # return empty dict if no tag found in file\n",
    "    # else, return the sentences with the labels\n",
    "    if count==0:\n",
    "        return {}\n",
    "    else:\n",
    "        #print(\"sent_label:\", sent_label)\n",
    "        return sent_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TrainingData(filenames):\n",
    "    train_files = []\n",
    "    train_sent = []\n",
    "    train_labels = []\n",
    "    \n",
    "    train_df = pd.DataFrame()\n",
    "    for file in filenames:\n",
    "        file_data = process_file(file=file)\n",
    "        for i in range(0, len(file_data)):\n",
    "            train_files.append(file)\n",
    "            \n",
    "        for key, value in file_data.items():\n",
    "            train_sent.append(key)\n",
    "            train_labels.append(value)\n",
    "    \n",
    "    train_df = pd.DataFrame({'filename': train_files, 'sentence': train_sent, 'label': train_labels})\n",
    "    #print(train_df.head())\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = get_TrainingData(train_filenames)\n",
    "df_dev = get_TrainingData(dev_filenames)\n",
    "df_test = get_TrainingData(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\103-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\109-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\155-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\251-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\107-02.xml']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\335-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\149-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\366-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\181-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\180-01.xml']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\111-01.xml']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>Record date: 2067-11-24</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>HUNTINGTON EMERGENCY DEPT...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>THOMAS-YOSEF,JULIA   840-91-51-9            VI...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>This patient was seen with Dr. Earley.</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>The patient was interviewed</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "1  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "2  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "3  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "4  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "\n",
       "                                            sentence  label  \n",
       "0                            Record date: 2067-11-24  Other  \n",
       "1                       HUNTINGTON EMERGENCY DEPT...  Other  \n",
       "2  THOMAS-YOSEF,JULIA   840-91-51-9            VI...  Other  \n",
       "3             This patient was seen with Dr. Earley.  Other  \n",
       "4                       The patient was interviewed   Other  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>Record date: 2104-01-30</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>NEUROLOGY RESIDENT CONSULT NOTE</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>PATIENT NAME: Dalila Haynes</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>MRN: 78361343</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>DATE OF CONSULT: 1/30/04</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "1  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "2  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "3  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "4  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "\n",
       "                           sentence  label  \n",
       "0           Record date: 2104-01-30  Other  \n",
       "1  NEUROLOGY RESIDENT CONSULT NOTE   Other  \n",
       "2       PATIENT NAME: Dalila Haynes  Other  \n",
       "3                     MRN: 78361343  Other  \n",
       "4          DATE OF CONSULT: 1/30/04  Other  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>Record date: 2069-04-07</td>\n",
       "      <td>smoker.unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>Mr. Villegas is seen today.</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>I have not seen him since November.</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>About three weeks ago he stopped his Prednison...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...</td>\n",
       "      <td>he was gaining weight.</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "1  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "2  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "3  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "4  E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Fina...   \n",
       "\n",
       "                                            sentence           label  \n",
       "0                            Record date: 2069-04-07  smoker.unknown  \n",
       "1                        Mr. Villegas is seen today.           Other  \n",
       "2                I have not seen him since November.           Other  \n",
       "3  About three weeks ago he stopped his Prednison...           Other  \n",
       "4                             he was gaining weight.           Other  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               __label__Other Record date: 2067-11-24\n",
       "1    __label__Other                      HUNTINGTON...\n",
       "2    __label__Other THOMAS-YOSEF,JULIA   840-91-51-...\n",
       "3    __label__Other This patient was seen with Dr. ...\n",
       "4          __label__Other The patient was interviewed \n",
       "Name: formatted, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_flair = df_train[['label', 'sentence']].copy()\n",
    "df_train_flair['prefix'] = '__label__'\n",
    "df_train_flair = df_train_flair[['prefix', 'label', 'sentence']]\n",
    "df_train_flair['formatted'] = df_train_flair['prefix']+df_train_flair['label']+' '+df_train_flair['sentence']\n",
    "df_train_flair['formatted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             __label__Other Record date: 2104-01-30\n",
       "1    __label__Other NEUROLOGY RESIDENT CONSULT NOTE \n",
       "2         __label__Other PATIENT NAME: Dalila Haynes\n",
       "3                       __label__Other MRN: 78361343\n",
       "4            __label__Other DATE OF CONSULT: 1/30/04\n",
       "Name: formatted, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_flair = df_dev[['label', 'sentence']].copy()\n",
    "df_dev_flair['prefix'] = '__label__'\n",
    "df_dev_flair = df_dev_flair[['prefix', 'label', 'sentence']]\n",
    "df_dev_flair['formatted'] = df_dev_flair['prefix']+df_dev_flair['label']+' '+df_dev_flair['sentence']\n",
    "df_dev_flair['formatted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      __label__smoker.unknown Record date: 2069-04-07\n",
       "1           __label__Other Mr. Villegas is seen today.\n",
       "2    __label__Other I have not seen him since Novem...\n",
       "3    __label__Other About three weeks ago he stoppe...\n",
       "4                __label__Other he was gaining weight.\n",
       "Name: formatted, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_flair = df_test[['label', 'sentence']].copy()\n",
    "df_test_flair['prefix'] = '__label__'\n",
    "df_test_flair = df_test_flair[['prefix', 'label', 'sentence']]\n",
    "df_test_flair['formatted'] = df_test_flair['prefix']+df_test_flair['label']+' '+df_test_flair['sentence']\n",
    "df_test_flair['formatted'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_flair['formatted'].to_csv('E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\FLAIR_data\\\\train.txt', index=False, header=False)\n",
    "df_dev_flair['formatted'].to_csv('E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\FLAIR_data\\\\dev.txt', index=False, header=False)\n",
    "df_test_flair['formatted'].to_csv('E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\FLAIR_data\\\\test.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-11 12:54:20,842 Reading data from E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Final Project\\FLAIR_data\n",
      "2019-04-11 12:54:20,847 Train: E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Final Project\\FLAIR_data\\train.txt\n",
      "2019-04-11 12:54:20,851 Dev: E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Final Project\\FLAIR_data\\dev.txt\n",
      "2019-04-11 12:54:20,854 Test: E:\\Google Drive\\Berkeley\\Courses\\w266_NLP\\Final Project\\FLAIR_data\\test.txt\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path('E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\FLAIR_data\\\\')\n",
    "\n",
    "# load corpus containing training, test, and dev data\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_classification_corpus(data_folder,\n",
    "                                                                     test_file='test.txt',\n",
    "                                                                     dev_file='dev.txt',\n",
    "                                                                     train_file='train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create label dictionary\n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of word embeddings\n",
    "word_embeddings = [#WordEmbeddings('glove'),\n",
    "                   BertEmbeddings(),\n",
    "                   #FlairEmbeddings('pubmed-forward'),\n",
    "                   #FlairEmbeddings('pubmed-backward'),\n",
    "                   #CharacterEmbeddings()\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize document embedding by passing list of word embeddings\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-11 12:54:46,041 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-11 12:54:46,043 Evaluation method: MICRO_F1_SCORE\n",
      "2019-04-11 12:54:46,072 ----------------------------------------------------------------------------------------------------\n",
      "2019-04-11 12:54:49,627 epoch 1 - iter 0/1558 - loss 0.14745218\n",
      "2019-04-11 13:06:11,925 epoch 1 - iter 155/1558 - loss 0.02543820\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-648c6ec4b0c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0manneal_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m               max_epochs=20)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\trainers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, base_path, evaluation_metric, learning_rate, mini_batch_size, eval_mini_batch_size, max_epochs, anneal_factor, patience, anneal_against_train_loss, train_with_dev, monitor_train, embeddings_in_memory, checkpoint, save_final_model, anneal_with_restarts, test_mode, param_selection_mode, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mbatch_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\models\\text_classification_model.py\u001b[0m in \u001b[0;36mforward_loss\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\models\\text_classification_model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mtext_embedding_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m   1668\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1672\u001b[0m         \u001b[1;31m# first, sort sentences by number of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\flair\\embeddings.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m   1164\u001b[0m                     \u001b[0mall_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mlayer_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_indexes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m                         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_encoder_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m                         \u001b[0mall_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "trainer.train('E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\FLAIR_data\\\\BERT',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
