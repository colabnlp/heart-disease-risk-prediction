{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15282499900818875963\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10193742398\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11351185109022645623\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the files for parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\220-05.xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codefolder = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "datafolders = ['\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\','\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\']\n",
    "filenames = []\n",
    "\n",
    "for folder in datafolders:\n",
    "    for file in os.listdir(str(codefolder)+folder):\n",
    "        filename=os.fsdecode(os.fsencode((str(codefolder)+folder+file)))\n",
    "        if filename.endswith(('.xml')):\n",
    "            filenames.append(filename)\n",
    "            \n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a 90/10 split\n",
    "split_index = int(len(filenames)*0.9)\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)\n",
    "\n",
    "train_filenames = filenames[:split_index]\n",
    "dev_filenames = filenames[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\103-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\109-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\155-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\251-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\107-02.xml']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\335-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\149-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\366-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\181-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\180-01.xml']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-01.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-02.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-03.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\110-04.xml',\n",
       " 'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\111-01.xml']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafolder = '\\\\data_all\\\\16_testing-RiskFactors-Complete\\\\'\n",
    "test_filenames = []\n",
    "\n",
    "for file in os.listdir(str(codefolder)+datafolder):\n",
    "    filename=os.fsdecode(os.fsencode((str(codefolder)+datafolder+file)))\n",
    "    if filename.endswith(('.xml')):\n",
    "        test_filenames.append(filename)\n",
    "\n",
    "test_filenames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to get tokens & their attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    offset = 0\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for token in tokens:\n",
    "        offset = text.find(token, offset)\n",
    "        start, end = offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "        start_indices.append(start)\n",
    "        end_indices.append(end)\n",
    "    return tokens, start_indices, end_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to generate IO Coding\n",
    "IO coding is a technique for extracting entities, where in the input sentence is tokenized and analyzed for occurences of words that belong to an entity of interest.  It follows the following scheme:\n",
    "\n",
    "* I - marks beginning/inside of the entity\n",
    "* O - marks that the token is NOT part of any entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_IO_Coding(file_path):\n",
    "    tree = ET.ElementTree(file=file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    text = root.find('TEXT').text.lower()\n",
    "    \n",
    "    tokens, start, end = spans(text)\n",
    "    \n",
    "    labels_list = []\n",
    "    label_start = []\n",
    "    label_end = []\n",
    "    \n",
    "    filename = []\n",
    "\n",
    "    for item in root.find(\"TAGS\"):\n",
    "        if item.tag == 'PHI':\n",
    "            pass\n",
    "        elif item.tag == 'SMOKER':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['status']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'FAMILY_HIST':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['indicator']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'MEDICATION':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['type1'] + \".\" + item.attrib['type2'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['indicator'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "\n",
    "        for sub_item in item.findall(item.tag):\n",
    "            if 'start' in sub_item.attrib.keys():\n",
    "                labels_list.append(label)\n",
    "                label_start.append(int(sub_item.attrib['start']))\n",
    "                label_end.append(int(sub_item.attrib['end']))\n",
    "                \n",
    "    io_labels = []\n",
    "\n",
    "    count = 0\n",
    "    while len(start) > count:\n",
    "        if start[count] in label_start:\n",
    "            label_start_index = label_start.index(start[count])\n",
    "            end_index = label_end[label_start_index]\n",
    "            word_label = labels_list[label_start_index]\n",
    "            phrase = text[start[count]:end_index]\n",
    "            phrase_tokens = nltk.word_tokenize(phrase)\n",
    "            for word in phrase_tokens:\n",
    "                io_labels.append(word_label)\n",
    "                count += 1\n",
    "        else:\n",
    "            io_labels.append(\"O\")\n",
    "            count += 1\n",
    "            \n",
    "    # build this list to hold name of the file the token belongs to\n",
    "    # this is for the purpose of evaluation of the model from test results\n",
    "    for i in range(0, len(tokens)):\n",
    "        filename.append(file_path)\n",
    "    \n",
    "    return filename, tokens, io_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIOCoding_data(filenames):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "\n",
    "    for file in filenames:\n",
    "        #print(\"processing file ... \", file)\n",
    "        filename, tokens, bio_labels = Generate_IO_Coding(file_path=file)\n",
    "\n",
    "        all_tokens.extend(tokens)\n",
    "        all_labels.extend(bio_labels)\n",
    "        all_filenames.extend(filename)  # this is for the purpose of validating test results\n",
    "        #print(\"finished processing file \", file, \"; and token length is \", len(all_tokens), \"; and label length: \", len(all_labels))\n",
    "        \n",
    "    return all_filenames, all_tokens, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(filenames, input_type=\"train\"):\n",
    "    files, tokens, labels = getIOCoding_data(filenames)\n",
    "    df = pd.DataFrame({'filename': files, 'token': tokens, 'label': labels})\n",
    "    \n",
    "    if (input_type == \"train\") or (input_type == \"dev\"):\n",
    "        le = LabelEncoder()\n",
    "        # Train - CoLA Format\n",
    "        df_bert = pd.DataFrame({'user_id':df.index, \n",
    "                                'label':le.fit_transform(df['label']), \n",
    "                                'alpha':['a']*df.shape[0], \n",
    "                                'text':df['token'].replace(r'\\n',' ', regex=True)})\n",
    "    \n",
    "    elif input_type == \"test\":\n",
    "        # Test -  CoLA Format\n",
    "        df_bert = pd.DataFrame({'id':df.index, \n",
    "                                'sentence':df['token'].replace(r'\\n', ' ', regex=True)})\n",
    "    \n",
    "    path = os.path.join(os.path.dirname(os.path.abspath('__file__')), \"Single_Model\")\n",
    "    if os.path.exists(path) == False:\n",
    "        os.makedirs(path)\n",
    "    df_bert.to_csv(os.path.join(path, input_type+\".tsv\"), sep='\\t', index=False, header=False)\n",
    "    return df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_types = [['train', train_filenames], ['dev', dev_filenames], ['test', test_filenames]]\n",
    "for input_type in input_types:\n",
    "    preprocess_input(input_type[1], input_type=input_type[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data for QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Google Drive\\\\Berkeley\\\\Courses\\\\w266_NLP\\\\Final Project\\\\Single_Model\\\\train.tsv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codefolder = os.path.dirname(os.path.abspath('__file__'))\n",
    "filename = os.fsdecode(os.fsencode((str(codefolder)+'\\\\Single_Model\\\\train.tsv')))\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>record</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>2067-11-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>huntington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1  2           3\n",
       "0  0  91  a      record\n",
       "1  1  91  a        date\n",
       "2  2  91  a           :\n",
       "3  3  91  a  2067-11-24\n",
       "4  4  91  a  huntington"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(filename, sep=\"\\t\", header=None)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[1].unique()) + 1 # zero-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
