{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0405 22:06:05.088888 16128 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "#import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "#from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4718195028315360765\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10193742398\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6725706880284593926\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:01:00.0, compute capability: 5.2\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT parameters and tokenizer\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get files for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "folders = ['\\\\data_all\\\\06_training-RiskFactors-Complete-Set1\\\\','\\\\data_all\\\\09_training-RiskFactors-Complete-Set2\\\\']\n",
    "filenames = []\n",
    "\n",
    "for folder in folders:\n",
    "    for file in os.listdir(str(wd)+folder):\n",
    "        filename=os.fsdecode(os.fsencode((str(wd)+folder+file)))\n",
    "        if filename.endswith(('.xml')):\n",
    "            filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to create training tokens and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module():\n",
    "    bert_module = hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "    [\n",
    "        tokenization_info[\"vocab_file\"],\n",
    "        tokenization_info[\"do_lower_case\"],\n",
    "    ])\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sarah\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0405 22:06:10.070299 16128 deprecation.py:323] From C:\\Users\\Sarah\\Anaconda3\\envs\\Tensorflow-GPU-Keras\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0405 22:06:13.824102 16128 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans(tokenizer, text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    offset = 0\n",
    "    start_indices = []\n",
    "    end_indices = []\n",
    "    for token in tokens:\n",
    "        offset = text.find(token, offset)\n",
    "        start, end = offset, offset+len(token)\n",
    "        offset += len(token)\n",
    "        start_indices.append(start)\n",
    "        end_indices.append(end)\n",
    "    return tokens, start_indices, end_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_input(tokenizer, file_path):\n",
    "    tree = ET.ElementTree(file=file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    text = root.find('TEXT').text.lower()\n",
    "    \n",
    "    tokens, start, end = spans(tokenizer, text)\n",
    "    \n",
    "    labels_list = []\n",
    "    label_start = []\n",
    "    label_end = []\n",
    "\n",
    "    for item in root.find(\"TAGS\"):\n",
    "        if item.tag == 'PHI':\n",
    "            pass\n",
    "        elif item.tag == 'SMOKER':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['status']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'FAMILY_HIST':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['indicator']).lower().replace(\" \", \"_\")\n",
    "        elif item.tag == 'MEDICATION':\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['type1'] + \".\" + item.attrib['type2'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "        else:\n",
    "            label = \"I-\" + (item.tag + \".\" + item.attrib['indicator'] + \".\" + item.attrib['time']).lower().replace(\" \", \"_\")\n",
    "\n",
    "        for sub_item in item.findall(item.tag):\n",
    "            if 'start' in sub_item.attrib.keys():\n",
    "                labels_list.append(label)\n",
    "                label_start.append(int(sub_item.attrib['start']))\n",
    "                label_end.append(int(sub_item.attrib['end']))\n",
    "                \n",
    "    io_labels = []\n",
    "\n",
    "    count = 0\n",
    "    while len(start) > count:\n",
    "        if start[count] in label_start:\n",
    "            label_start_index = label_start.index(start[count])\n",
    "            end_index = label_end[label_start_index]\n",
    "            word_label = labels_list[label_start_index]\n",
    "            phrase = text[start[count]:end_index]\n",
    "            phrase_tokens = nltk.word_tokenize(phrase)\n",
    "            for word in phrase_tokens:\n",
    "                io_labels.append(word_label)\n",
    "                count += 1\n",
    "        else:\n",
    "            io_labels.append(\"O\")\n",
    "            count += 1  \n",
    "    \n",
    "    tokens_bert = []\n",
    "    io_labels_bert = []\n",
    "    for token, label in zip(tokens, io_labels):\n",
    "        new_tokens = tokenizer.tokenize(token)\n",
    "        for token in new_tokens:\n",
    "            tokens_bert.append(token)\n",
    "            io_labels_bert.append(label)\n",
    "    tokens_bert.insert(0, '[CLS]')\n",
    "    tokens_bert.append('[SEP]')\n",
    "    io_labels_bert.insert(0, \"O\")\n",
    "    io_labels_bert.append(\"O\")\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens_bert)\n",
    "    return tokens_bert, io_labels_bert, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data from .xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "train_labels = []\n",
    "train_input_ids = []\n",
    "\n",
    "split_index = int(len(filenames)*0.75)\n",
    "random.seed(42)\n",
    "random.shuffle(filenames)\n",
    "for file in filenames[:split_index]:\n",
    "    tokens, io_labels, input_ids = create_training_input(tokenizer, file)\n",
    "\n",
    "    train_tokens.extend(tokens)\n",
    "    train_labels.extend(io_labels)\n",
    "    train_input_ids.extend(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tokens = []\n",
    "dev_labels = []\n",
    "dev_input_ids = []\n",
    "\n",
    "for file in filenames[split_index:]:\n",
    "    tokens, io_labels, input_ids = create_training_input(tokenizer, file)\n",
    "    \n",
    "    dev_tokens.extend(tokens)\n",
    "    dev_labels.extend(io_labels)\n",
    "    dev_input_ids.extend(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BERT compatible features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {}\n",
    "int_to_label = {}\n",
    "\n",
    "unique_labels = list(set(dev_labels+train_labels))\n",
    "for i in range(len(unique_labels)):\n",
    "    label_to_int[unique_labels[i]] = i\n",
    "    int_to_label[i] = unique_labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label_to_int(labels, dictionary):\n",
    "    train_classes = []\n",
    "    for label in labels:\n",
    "        train_classes.append(dictionary[label])\n",
    "    return train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = convert_label_to_int(train_labels, label_to_int)\n",
    "dev_classes = convert_label_to_int(dev_labels, label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input_masks = np.array([1] * len(train_classes))\n",
    "# dev_input_masks = np.array([1] * len(dev_classes))\n",
    "\n",
    "# train_segment_ids = np.array([0] * len(train_classes))\n",
    "# dev_segment_ids = np.array([0] * len(train_classes))\n",
    "\n",
    "# train_labels = np_utils.to_categorical(train_classes)\n",
    "# dev_labels = np_utils.to_categorical(dev_classes)\n",
    "\n",
    "# train_input_ids = np.array(train_input_ids)\n",
    "# dev_input_ids = np.array(dev_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seqs(classes, ids, dictionary, max_seq_length=100):\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "\n",
    "    start = 0\n",
    "    end = max_seq_length\n",
    "    while start < len(classes):\n",
    "        while (classes[start:end][-1] != dictionary[\"O\"]):\n",
    "            end -= 1\n",
    "            if end == start:\n",
    "                end += max_seq_length\n",
    "                break\n",
    "        seq_segment_ids = [0] * max_seq_length\n",
    "        seq_classes = classes[start:end] + [dictionary[\"O\"]] * (max_seq_length-len(classes[start:end]))\n",
    "        seq_ids = ids[start:end] + [0] * (max_seq_length-len(ids[start:end]))\n",
    "        seq_mask = [1] * len(ids[start:end]) + [0] * (max_seq_length-len(ids[start:end]))\n",
    "\n",
    "        input_ids.append(seq_ids)\n",
    "        input_masks.append(seq_mask)\n",
    "        segment_ids.append(seq_segment_ids)\n",
    "        labels.append(seq_classes)\n",
    "\n",
    "        start = end\n",
    "        end += max_seq_length\n",
    "        \n",
    "    return (\n",
    "        np.array(input_ids), \n",
    "        np.array(input_masks), \n",
    "        np.array(segment_ids), \n",
    "        #np.array(labels)\n",
    "        #np.array(labels).reshape(-1,1),\n",
    "        np_utils.to_categorical(np.array(labels)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "# Create features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = create_seqs(train_classes, train_input_ids, label_to_int, max_seq_length)\n",
    "(dev_input_ids, dev_input_masks, dev_segment_ids, dev_labels) = create_seqs(dev_classes, dev_input_ids, label_to_int, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6475, 100) (6475, 100, 93)\n"
     ]
    }
   ],
   "source": [
    "print(train_input_ids.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "\n",
    "        trainable_vars = self.bert.variables\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "            \n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBase:\n",
    "    @staticmethod\n",
    "    def build(max_seq_length):\n",
    "        in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "        in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "        in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "        bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "        bert_output = BertLayer(n_fine_tune_layers=5)(bert_inputs)\n",
    "        return bert_output, bert_inputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseHead:\n",
    "    @staticmethod\n",
    "    def build(bert_base):\n",
    "        headModel = bert_base\n",
    "        headModel = tf.keras.layers.Flatten(name=\"flatten\")(headModel)\n",
    "        headModel = tf.keras.layers.Dense(9300, activation=\"relu\")(headModel)\n",
    "        headModel = tf.keras.layers.Reshape((-1,93))(headModel)\n",
    "        headModel = tf.keras.layers.Dense(93, activation=\"softmax\")(headModel)\n",
    "        \n",
    "        return headModel\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_acc(y_true, y_pred):\n",
    "    return tf.keras.metrics.categorical_accuracy(K.flatten(y_true), K.flatten(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0405 22:17:57.175815 16128 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0405 22:18:00.563734 16128 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_8 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 768)          0           bert_layer_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 9300)         7151700     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, None, 93)     0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, None, 93)     8742        reshape_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 117,265,332\n",
      "Trainable params: 10,111,866\n",
      "Non-trainable params: 107,153,466\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_output, bert_inputs = BERTBase.build(max_seq_length)\n",
    "dense_head = DenseHead.build(bert_output)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=bert_inputs, outputs=dense_head)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[cat_acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6475 samples, validate on 2243 samples\n",
      "6475/6475 [==============================] - 71s 11ms/sample - loss: 0.4862 - cat_acc: 0.0000e+00 - val_loss: 0.3817 - val_cat_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ceea1fe4e0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    \n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], dev_labels),\n",
    "    epochs=3,\n",
    "    batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "preds = model.predict([dev_input_ids, dev_input_masks, dev_segment_ids])\n",
    "\n",
    "def convert_pred_to_label(preds):\n",
    "    out = []\n",
    "    for pred in preds:\n",
    "        out_i = []\n",
    "        for p in pred:\n",
    "            p_i = np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in preds:\n",
    "    for j in i:\n",
    "        label = int_to_label[np.argmax(j)]\n",
    "        if label != 'O':\n",
    "            print(int_to_label[np.argmax(j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_label[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {}\n",
    "int_to_label = {}\n",
    "\n",
    "def convert_label_to_int(labels, dictionary):\n",
    "    train_classes = []\n",
    "    for label in labels:\n",
    "        train_classes.append(dictionary[label])\n",
    "    return train_classes\n",
    "\n",
    "\n",
    "4768/4768 [==============================] - 64s 13ms/step\n",
    "idx2tag = {i: w for w, i in tags2index.items()}\n",
    "def pred2label(pred):\n",
    "    out = []\n",
    "    for pred_i in pred:\n",
    "        out_i = []\n",
    "        for p in pred_i:\n",
    "            p_i = np.argmax(p)\n",
    "            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n",
    "        out.append(out_i)\n",
    "    return out\n",
    "\n",
    "    \n",
    "pred_labels = pred2label(test_pred)\n",
    "test_labels = test2label(y_te[:149*32])\n",
    "print(classification_report(test_labels, pred_labels))\n",
    "               precision   recall  f1-score   support\n",
    "\n",
    "        org       0.69      0.66      0.68      2061\n",
    "        tim       0.88      0.84      0.86      2148\n",
    "        gpe       0.95      0.93      0.94      1591\n",
    "        per       0.75      0.80      0.77      1677\n",
    "        geo       0.85      0.89      0.87      3720\n",
    "        art       0.23      0.14      0.18        49\n",
    "        eve       0.33      0.33      0.33        33\n",
    "        nat       0.47      0.36      0.41        22\n",
    "\n",
    "avg / total       0.82      0.82      0.82     11301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build modelhttp://localhost:8888/notebooks/Final%20Project/Single%20Model.ipynb#SCRATCH\n",
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "    flatten = tf.keras.layers.Flatten(name=\"flatten\")(bert_output)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(flatten)\n",
    "    pred = tf.keras.layers.Dense(93, activation='softmax')(dense) # number of prediction nodes should equal number of unique labels\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(max_seq_length)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([dev_input_ids, dev_input_masks, dev_segment_ids], dev_labels),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
